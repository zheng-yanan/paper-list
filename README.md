## Disentangled Representation Learning

Disentangled representations learning is an unsupervised learning technique that disentangles each feature into narrowly defined variables and encodes them as separated dimensions.
Its goal is to mimic the quick intuition process of a human, using both “high” and “low” dimension reasoning.
This is a collection of milestone papers on disentangled representation learning since 2016.

* **[2016] InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets [[paper]](https://arxiv.org/pdf/1606.03657v1.pdf)**
* **[NIPS2016] Disentangling factors of variation in deep representations using adversarial training [[paper]](https://arxiv.org/pdf/1611.03383.pdf)**
* **[ICLR2017] Deep Variational Information Bottleneck [[paper]](https://arxiv.org/abs/1612.00410)**
* **[ICLR2017] Beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework [[paper]](https://openreview.net/forum?id=Sy2fzU9gl) [[code]](https://github.com/katalinic/betaVAE)**
* **[NIPS2017] The Beta-VAE’s Implicit Prior [[paper]](http://bayesiandeeplearning.org/2017/papers/66.pdf)**
* **[NIPS2017] Understanding Disentangling in Beta-VAE [[paper]](https://arxiv.org/pdf/1804.03599.pdf)**
* **[NIPS2017] Disentangling the independently controllable factors of variation by interacting with the world [[paper]](https://arxiv.org/pdf/1802.09484.pdf)**
* **[NIPS2017] Disentangled Representations for Manipulation of Sentiment in Text [[paper]](http://www.kageback.se/2017/12/09/disentangled-representations-for-manipulation-of-sentiment-in-text/)**
* **[NIPS2017] Quantifying the Effects of Enforcing Disentanglement on Variational Autoencoders [[paper]](https://arxiv.org/abs/1711.09159)**
* **[NIPS2017] Semantically Decomposing the Latent Spaces of Adversarial Generative Network [[paper]](https://arxiv.org/abs/1705.07904)**
* **[NIPS2017] Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data [[paper]](https://arxiv.org/pdf/1709.07902.pdf) [[code]](https://github.com/wnhsu/ScalableFHVAE)**
* **[NIPS2017] Learning Disentangled Representations with Semi-Supervised Deep Generative Models [[paper]](https://arxiv.org/pdf/1706.00400.pdf)**
* **[Workshop on NIPS2018] Recent Advances in Autoencoder-Based Representation Learning [[paper]](http://bayesiandeeplearning.org/2018/papers/151.pdf?fbclid=IwAR0AKPuAsCFFsTCJ52o6-BkJebR9UuURnesksd1wf5QfLvuU2LBetc7moKc)**
* **[ICML2018] Disentangling by Factorising [[paper]](https://arxiv.org/pdf/1802.05983.pdf)**
* **[NIPS2018] Isolating Sources of Disentanglement in VAEs [[paper]](https://arxiv.org/pdf/1802.04942v5.pdf) [[code]](https://github.com/rtqichen/beta-tcvae)**
* **[ICML2018] Fixing a Broken ELBO [[paper]](https://arxiv.org/pdf/1711.00464.pdf)**
* **[2018] Auto-Encoding Total Correlation Explanation [[paper]](https://arxiv.org/pdf/1802.05822v1.pdf)**
* **[2018] On the Latent Space of Wasserstein Auto-Encoders [[paper]](https://arxiv.org/pdf/1802.03761.pdf)**
* **[Workshop on ICLR2018] Learning Disentangled Representations with Wasserstein Auto-encoders [[paper]](https://openreview.net/pdf?id=Hy79-UJPM)**
* **[ICLR2018] A Framework for the Quantitative Evaluation of Disentangled Representations [[paper]](https://openreview.net/pdf?id=By-7dz-AZ)**
* **[Workshop on ICLR2018] Rethinking Style and Content Disentanglement in Variational Auto-encoders [[paper]](https://openreview.net/pdf?id=B1rQtwJDG)**
* **[ICML2018] Disentangled Sequential Autoencoder [[paper]](https://arxiv.org/pdf/1803.02991.pdf)**
* **[ICML2018] Learning Independent Causal Mechanisms [[paper]](https://arxiv.org/pdf/1712.00961.pdf)**
* **[ICLR2019] Learning deep representations by mutual information estimation and maximization [[paper]](https://arxiv.org/abs/1808.06670) [[code]](https://github.com/rdevon/DIM)**
* **[ICML2019] Disentangling Disentanglement in Variational Autoencoders [[paper]](https://arxiv.org/pdf/1812.02833v3.pdf)**
* **[AISTATS2019] Structured Disentangled Representations [[paper]](https://arxiv.org/pdf/1804.02086.pdf)**
* **[2019] Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations [[paper]](https://arxiv.org/pdf/1811.12359v3.pdf)**



## Variational Neural Models 


* **[ICLR2014] Auto-Encoding Variational Bayes [[paper]](https://arxiv.org/pdf/1312.6114.pdf)**
* **[ICLR2015] Importance Weighted Autoencoders [[paper]](https://arxiv.org/abs/1509.00519)**
* **[Workshop on ICLR2015] Variational Recurrent Auto-Encoders [[paper]](https://arxiv.org/abs/1412.6581)**
* **[NIPS2015] A Recurrent Latent Variable Model for Sequential Data [[paper]](http://arxiv.org/abs/1506.02216) [[code]](https://github.com/jych/nips2015_vrnn)**
* **[ICLR2016] Generating Sentences from a Continuous Space [[paper]](https://arxiv.org/abs/1511.06349)**
* **[NIPS2016] Ladder Variational Autoencoders [[paper]](https://arxiv.org/pdf/1602.02282.pdf)**
* **[NIPS2016] Improving Variational Autoencoders with Inverse Autoregressive Flow [[paper]](https://papers.nips.cc/paper/6581-improved-variational-inference-with-inverse-autoregressive-flow.pdf)**
* **[Workshop on ICLR2017] Re-interpreting Importance Weighted Autoencoders [[paper]](https://openreview.net/pdf?id=Syw2ZgrFx)**
* **[ICML2017] Improved Variational Autoencoders for Text Modeling using Dilated Convolutions [[paper]](https://arxiv.org/abs/1702.08139)**
* **[AAAI2017] Variational Autoencoder for Semi-Supervised Text Classification [[paper]](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14299/14261)**
* **[ICLR2017] Variational Lossy Autoencoder [[paper]](https://arxiv.org/abs/1611.02731)**
* **[ICLR2017] Beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework [[paper]](https://openreview.net/forum?id=Sy2fzU9gl) [[code]](https://github.com/katalinic/betaVAE)**
* **[NIPS2017] InfoVAE: Information Maximizing Variational Autoencoders [[paper]](https://arxiv.org/abs/1706.02262)**



## Neural Dialogue Generation

* **[Workshop in ICML2015] A Neural Conversational Model [[paper]](https://arxiv.org/pdf/1506.05869.pdf)** 
* **[2016] A Persona-Based Neural Conversation Model [[paper]](https://arxiv.org/pdf/1603.06155v2.pdf)**
* **[2016] A Diversity-Promoting Objective Function for Neural Conversation Models [[paper]](https://arxiv.org/pdf/1510.03055v3.pdf)**
* **[2016] How NOT To Evaluate Your Dialogue System: An Empirical Study ofUnsupervised Evaluation Metrics for Dialogue Response Generation [[paper]](https://arxiv.org/pdf/1603.08023v1.pdf)**
* **[2016] A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues [[paper]](https://arxiv.org/pdf/1605.06069v3.pdf)**
* **[2016] Deep Reinforcement Learning for Dialogue Generation [[paper]](https://arxiv.org/pdf/1606.01541.pdf)**
* **[2016] Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models [[paper]](https://arxiv.org/pdf/1507.04808.pdf)**
* **[ACL2016] A Persona-Based Neural Conversation Model [[paper]](http://www.aclweb.org/anthology/P16-1094)**
* **[ACL2017] A Conditional Variational Framework for Dialog Generation [[paper]](https://arxiv.org/pdf/1705.00316.pdf)** 
* **[ICLR2018] Towards Interpretable Chit-Chat: Open Domain Dialogue Generation With Dialogue Acts [[paper]](https://openreview.net/pdf?id=Bym0cU1CZ)** 
* **[IJCAI2017] Exploring Personalized Neural Conversational Models [[paper]](https://www.ijcai.org/proceedings/2017/0521.pdf)**
* **[SIGIR2017] Personalized Response Generation via Domain adaptation [[paper]](https://dl.acm.org/citation.cfm?id=3080706)**
* **[AAAI2017] Topic Aware Neural Response Generation [[paper]](https://arxiv.org/pdf/1606.08340.pdf)**
* **[ACL2018] Personalizing Dialogue Agents: I have a dog, do you have pets too? [[paper]](http://aclweb.org/anthology/P18-1205)**
* **[ACL2018] Generating Informative Responses with Controlled Sentence Function [[paper]](http://www.aclweb.org/anthology/P18-1139)**
* **[AAAI2018] Improving Variational Encoder-Decoders in Dialogue Generation [[paper]](https://arxiv.org/abs/1802.02032)**
* **[AAAI2018] Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory [[paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16455/15753)**
* **[SIGIR2018] Chat More: Deepening and Widening the Chatting Topic via A Deep Model [[paper]](http://coai.cs.tsinghua.edu.cn/hml/media/files/2018SIGIR_Wangwenjie.pdf)**
* **[WWW2018] Hierarchical Variational Memory Network for Dialogue Generation [[paper]](https://dl.acm.org/citation.cfm?id=3186077)**
* **[NAACL2018] Automatic Dialogue Generation with Expressed Emotions [[paper]](http://aclweb.org/anthology/N18-2008)**
* **[Workshop on ACL2019] Augmenting Neural Response Generation with Context-Aware Topical Attention [[paper]](https://arxiv.org/pdf/1811.01063.pdf)**

 

